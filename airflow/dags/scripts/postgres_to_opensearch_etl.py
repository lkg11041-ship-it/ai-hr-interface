"""
PostgreSQL to OpenSearch ETL Script
Strategy: Full Reload - Extract recruitment data and generate vector embeddings
Features: Batch processing, error handling, execution logging

âš ï¸ ìš´ì˜ í™˜ê²½ ë°°í¬ ì‹œ ìˆ˜ì • í•„ìš”:
1. OPENSEARCH_URL: ì™¸ë¶€ OpenSearch í´ëŸ¬ìŠ¤í„° URLë¡œ ë³€ê²½ (ì˜ˆ: http://10.0.0.13:9200)
2. OPENSEARCH_USER/PASSWORD: ë³´ì•ˆì´ í™œì„±í™”ëœ ê²½ìš° ì¸ì¦ ì •ë³´ ì¶”ê°€
3. OS_INDEX: ìš´ì˜ ì¸ë±ìŠ¤ëª… í™•ì¸ (ê¸°ë³¸: candidates)
4. BATCH_SIZE: ë°ì´í„° ê·œëª¨ì— ë”°ë¼ ì¡°ì • (ê¸°ë³¸: 500)
5. INDEX ì‚­ì œ/ì¬ìƒì„± ì „ëµ: ìš´ì˜ì—ì„œëŠ” alias ì „í™˜ ë°©ì‹ ê¶Œì¥
"""

import os
import sys
import traceback
from datetime import datetime
from typing import List, Dict, Any

import psycopg2
from psycopg2.extras import execute_batch
from opensearchpy import OpenSearch
from opensearchpy.helpers import bulk

# ==================== Environment Variables ====================
# PostgreSQL Connection (Source)
PG_HOST = os.getenv("POSTGRES_HOST")
PG_PORT = int(os.getenv("POSTGRES_PORT", "5432"))
PG_DB = os.getenv("POSTGRES_DB")
PG_USER = os.getenv("POSTGRES_USER")
PG_PASSWORD = os.getenv("POSTGRES_PASSWORD")

# OpenSearch Connection (Target)
# ğŸ”§ PRODUCTION: ì™¸ë¶€ OpenSearch URLë¡œ ë³€ê²½ (ì˜ˆ: http://10.0.0.13:9200)
OS_URL = os.getenv("OPENSEARCH_URL", "http://opensearch:9200")
OS_INDEX = os.getenv("OS_CAND_INDEX", "candidates")

# ğŸ”§ PRODUCTION: ë³´ì•ˆì´ í™œì„±í™”ëœ ê²½ìš° ì¸ì¦ ì •ë³´ ì¶”ê°€
OS_USER = os.getenv("OPENSEARCH_USER", None)  # ìš´ì˜: admin ë“±
OS_PASSWORD = os.getenv("OPENSEARCH_PASSWORD", None)  # ìš´ì˜: ë¹„ë°€ë²ˆí˜¸ ì„¤ì •

# Embedding Model Configuration
EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
EMBED_DIM = int(os.getenv("EMBED_DIM", "384"))

# Constants
DAG_ID = "etl_postgres_to_opensearch_daily"
BATCH_SIZE = 500  # ğŸ”§ PRODUCTION: ëŒ€ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° 1000~2000ìœ¼ë¡œ ì¦ê°€ ê°€ëŠ¥
TARGET_SCHEMA = "app"

# Global state
_extraction_data = {
    'rows': [],
    'row_count': 0,
    'run_id': None,
    'started_at': None,
}

# Embedding model cache
_embed_model = None


def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
    global _embed_model
    if _embed_model is not None:
        return _embed_model

    try:
        from sentence_transformers import SentenceTransformer
        print(f"Loading embedding model: {EMBED_MODEL}")
        _embed_model = SentenceTransformer(EMBED_MODEL)
        return _embed_model
    except Exception as e:
        print(f"WARNING: Failed to load embedding model: {str(e)}")
        return None


def generate_embeddings(texts: List[str]) -> List[List[float]]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    model = get_embedding_model()

    if model:
        import numpy as np
        vecs = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)
        return np.asarray(vecs, dtype="float32").tolist()
    else:
        # Fallback: ëœë¤ ë²¡í„° (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
        print("WARNING: Using random embeddings (model not available)")
        import numpy as np
        vecs = []
        for text in texts:
            rng = np.random.default_rng(abs(hash(text)) % (2**32))
            v = rng.standard_normal(EMBED_DIM).astype("float32")
            v /= max(1e-6, np.linalg.norm(v))
            vecs.append(v.tolist())
        return vecs


def get_postgres_connection():
    """PostgreSQL ì—°ê²° ìƒì„±"""
    return psycopg2.connect(
        host=PG_HOST,
        port=PG_PORT,
        dbname=PG_DB,
        user=PG_USER,
        password=PG_PASSWORD
    )


def get_opensearch_client():
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    # ğŸ”§ PRODUCTION: ë³´ì•ˆì´ í™œì„±í™”ëœ ê²½ìš° http_auth ì¶”ê°€
    if OS_USER and OS_PASSWORD:
        return OpenSearch(
            hosts=[OS_URL],
            http_auth=(OS_USER, OS_PASSWORD),
            use_ssl=True,  # HTTPS ì‚¬ìš© ì‹œ
            verify_certs=True,  # ì¸ì¦ì„œ ê²€ì¦ (ìš´ì˜: True ê¶Œì¥)
            timeout=30
        )
    else:
        # Local/Dev í™˜ê²½ (ë³´ì•ˆ ë¹„í™œì„±í™”)
        return OpenSearch(hosts=[OS_URL], timeout=30)


def log_run_start():
    """ETL ì‹œì‘ ë¡œê·¸ ê¸°ë¡"""
    print(f"[{datetime.now()}] Starting ETL run: {DAG_ID}")

    if not all([PG_HOST, PG_USER, PG_PASSWORD, PG_DB, OS_URL]):
        raise ValueError("Required environment variables not set")

    _extraction_data['started_at'] = datetime.now()

    with get_postgres_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(f"""
                INSERT INTO {TARGET_SCHEMA}.run_history
                (dag_id, task_id, source_table, target_table, status, started_at)
                VALUES (%s, %s, %s, %s, %s, %s)
                RETURNING run_id
            """, (
                DAG_ID,
                'log_start',
                f"{TARGET_SCHEMA}.recruitment",
                f"OpenSearch.{OS_INDEX}",
                'IN_PROGRESS',
                _extraction_data['started_at']
            ))
            _extraction_data['run_id'] = cur.fetchone()[0]
            conn.commit()

    print(f"Run ID: {_extraction_data['run_id']}")
    return _extraction_data['run_id']


def extract_from_postgres():
    """PostgreSQLì—ì„œ ì±„ìš© ë°ì´í„° ì¶”ì¶œ"""
    print(f"[{datetime.now()}] Extracting data from PostgreSQL: {TARGET_SCHEMA}.recruitment")

    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cur:
                # recruitment í…Œì´ë¸”ì—ì„œ ì „ì²´ ë°ì´í„° ì¡°íšŒ
                select_sql = f"""
                    SELECT
                        id,
                        candidate_id,
                        full_name,
                        gender,
                        birth_year,
                        education_level,
                        years_experience,
                        industry,
                        role_title,
                        resume_text,
                        source_updated_at,
                        load_date
                    FROM {TARGET_SCHEMA}.recruitment
                    ORDER BY id
                """

                print(f"Executing: {select_sql}")
                cur.execute(select_sql)
                rows = cur.fetchall()

                _extraction_data['rows'] = rows
                _extraction_data['row_count'] = len(rows)

                print(f"Extracted {len(rows)} rows from PostgreSQL")
                return len(rows)

    except Exception as e:
        log_error('extract_from_postgres', 'EXTRACT', e, f"{TARGET_SCHEMA}.recruitment", None)
        raise


def transform_and_index_to_opensearch():
    """ë°ì´í„° ë³€í™˜ ë° OpenSearch ì¸ë±ì‹±"""
    print(f"[{datetime.now()}] Transforming data and indexing to OpenSearch: {OS_INDEX}")

    rows = _extraction_data['rows']
    if not rows:
        print("No data to index")
        return 0

    try:
        # 1. ì„ë² ë”© í…ìŠ¤íŠ¸ ì¤€ë¹„ (resume_text í•„ë“œ ì‚¬ìš©)
        print("Preparing text for embeddings...")
        texts = []
        for row in rows:
            resume_text = row[9] or ""  # resume_text (index 9)
            texts.append(resume_text)

        # 2. ì„ë² ë”© ìƒì„±
        print(f"Generating embeddings for {len(texts)} documents...")
        embeddings = generate_embeddings(texts)

        # 3. OpenSearch ë¬¸ì„œ ì¤€ë¹„
        print("Preparing OpenSearch documents...")
        os_client = get_opensearch_client()

        # âš ï¸ PRODUCTION WARNING: ì¸ë±ìŠ¤ ì‚­ì œ/ì¬ìƒì„± ì „ëµ
        # í˜„ì¬: Full Reload (ì¸ë±ìŠ¤ ì‚­ì œ â†’ ì¬ìƒì„± â†’ ë°ì´í„° ì ì¬)
        # ìš´ì˜ ê¶Œì¥: Blue-Green ë°°í¬ (alias ì „í™˜)
        #   1. ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (ì˜ˆ: candidates_20251006)
        #   2. ë°ì´í„° ì ì¬
        #   3. alias 'candidates'ë¥¼ ìƒˆ ì¸ë±ìŠ¤ë¡œ ì „í™˜
        #   4. ì´ì „ ì¸ë±ìŠ¤ ì‚­ì œ
        #
        # ğŸ”§ PRODUCTION: ì•„ë˜ ì£¼ì„ í•´ì œí•˜ê³  alias ë¡œì§ìœ¼ë¡œ ë³€ê²½ ê¶Œì¥
        # new_index = f"{OS_INDEX}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        if os_client.indices.exists(index=OS_INDEX):
            print(f"Deleting existing index: {OS_INDEX}")
            os_client.indices.delete(index=OS_INDEX)

        # ì¸ë±ìŠ¤ ìƒì„± (ë§¤í•‘ í¬í•¨)
        print(f"Creating index with mapping: {OS_INDEX}")
        index_body = {
            "settings": {
                "index.knn": True,
                # ğŸ”§ PRODUCTION: ìƒ¤ë“œ/ë ˆí”Œë¦¬ì¹´ ìˆ˜ë¥¼ í´ëŸ¬ìŠ¤í„° ê·œëª¨ì— ë§ê²Œ ì¡°ì •
                "number_of_shards": 1,      # ìš´ì˜: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ 3~5ê°œ
                "number_of_replicas": 0     # ìš´ì˜: ê³ ê°€ìš©ì„±ì„ ìœ„í•´ 1~2ê°œ
            },
            "mappings": {
                "properties": {
                    "candidate_id": {"type": "keyword"},
                    "full_name": {"type": "text"},
                    "gender": {"type": "keyword"},
                    "birth_year": {"type": "integer"},
                    "education_level": {"type": "keyword"},
                    "years_experience": {"type": "integer"},
                    "industry": {"type": "keyword"},
                    "role_title": {"type": "text"},
                    "resume_text": {"type": "text"},
                    "resume_embedding": {
                        "type": "knn_vector",
                        "dimension": EMBED_DIM,
                        "method": {
                            "engine": "lucene",
                            "name": "hnsw",
                            "space_type": "cosinesimil"
                        }
                    },
                    "source_updated_at": {"type": "date"},
                    "load_date": {"type": "date"},
                    "indexed_at": {"type": "date"}
                }
            }
        }
        os_client.indices.create(index=OS_INDEX, body=index_body)

        # 4. Bulk ì¸ë±ì‹±
        print(f"Bulk indexing {len(rows)} documents...")
        actions = []
        for row, embedding in zip(rows, embeddings):
            doc = {
                "_index": OS_INDEX,
                "_id": str(row[0]),  # idë¥¼ ë¬¸ì„œ IDë¡œ ì‚¬ìš©
                "_source": {
                    "candidate_id": row[1],
                    "full_name": row[2],
                    "gender": row[3],
                    "birth_year": row[4],
                    "education_level": row[5],
                    "years_experience": row[6],
                    "industry": row[7],
                    "role_title": row[8],
                    "resume_text": row[9],
                    "resume_embedding": embedding,
                    "source_updated_at": row[10].isoformat() if row[10] else None,
                    "load_date": row[11].isoformat() if row[11] else None,
                    "indexed_at": datetime.now().isoformat()
                }
            }
            actions.append(doc)

        # Bulk insert
        success, failed = bulk(os_client, actions, chunk_size=BATCH_SIZE, raise_on_error=False)

        print(f"Successfully indexed {success} documents to OpenSearch")
        if failed:
            print(f"WARNING: Failed to index {len(failed)} documents")

        # 5. ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
        os_client.indices.refresh(index=OS_INDEX)

        return success

    except Exception as e:
        log_error('transform_and_index', 'LOAD', e, f"{TARGET_SCHEMA}.recruitment", OS_INDEX)
        raise


def log_run_success():
    """ETL ì„±ê³µ ë¡œê·¸ ê¸°ë¡"""
    print(f"[{datetime.now()}] Logging ETL success")

    finished_at = datetime.now()
    duration_ms = int((finished_at - _extraction_data['started_at']).total_seconds() * 1000)

    with get_postgres_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(f"""
                UPDATE {TARGET_SCHEMA}.run_history
                SET status = %s,
                    rows_read = %s,
                    rows_written = %s,
                    finished_at = %s,
                    duration_ms = %s,
                    message = %s
                WHERE run_id = %s
            """, (
                'SUCCESS',
                _extraction_data['row_count'],
                _extraction_data['row_count'],
                finished_at,
                duration_ms,
                f"Successfully indexed {_extraction_data['row_count']} documents to OpenSearch",
                _extraction_data['run_id']
            ))
            conn.commit()

    print(f"ETL completed successfully. Duration: {duration_ms}ms")


def log_run_failure():
    """ETL ì‹¤íŒ¨ ë¡œê·¸ ê¸°ë¡"""
    print(f"[{datetime.now()}] Logging ETL failure")

    finished_at = datetime.now()
    duration_ms = int((finished_at - _extraction_data['started_at']).total_seconds() * 1000)

    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(f"""
                    UPDATE {TARGET_SCHEMA}.run_history
                    SET status = %s,
                        finished_at = %s,
                        duration_ms = %s,
                        message = %s
                    WHERE run_id = %s
                """, (
                    'FAILED',
                    finished_at,
                    duration_ms,
                    "ETL pipeline failed - see error_log for details",
                    _extraction_data['run_id']
                ))
                conn.commit()
    except Exception as e:
        print(f"Error logging failure: {str(e)}")


def log_error(task_id: str, step: str, error: Exception, source_table: str, target_table: str):
    """ì—ëŸ¬ ë¡œê·¸ ì ì¬"""
    error_message = str(error)
    error_class = type(error).__name__
    stack_trace = traceback.format_exc()

    print(f"ERROR in {task_id}/{step}: {error_message}")
    print(stack_trace)

    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(f"""
                    INSERT INTO {TARGET_SCHEMA}.error_log
                    (run_id, dag_id, task_id, step, error_class, error_message,
                     stacktrace, source_table, target_table, error_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    _extraction_data.get('run_id'),
                    DAG_ID,
                    task_id,
                    step,
                    error_class,
                    error_message[:500],
                    stack_trace[:2000],
                    source_table,
                    target_table,
                    datetime.now()
                ))
                conn.commit()
                print("Error logged to error_log table")
    except Exception as log_error:
        print(f"Failed to log error: {str(log_error)}")


if __name__ == "__main__":
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
    try:
        log_run_start()
        extract_from_postgres()
        transform_and_index_to_opensearch()
        log_run_success()
        print("ETL completed successfully")
    except Exception as e:
        log_run_failure()
        print(f"ETL failed: {str(e)}")
        sys.exit(1)
